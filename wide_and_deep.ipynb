{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataset(name:str, n:int=10):\n",
    "\n",
    "    x1 = np.random.randint(0,255,[n,50])\n",
    "    x2 = np.random.randint(0,255,[n,50])\n",
    "\n",
    "    y = np.sum(x1*x2, axis=1, keepdims=True)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate((y, x1, x2), axis=1))\n",
    "\n",
    "    # first column is \"y\"\n",
    "    df.rename(columns={0:'y'}, inplace=True)\n",
    "\n",
    "    df.to_csv(name, sep='\\t', header=None)\n",
    "\n",
    "make_dataset('dummy_train.csv', 1000)\n",
    "make_dataset('dummy_test.csv', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Concatenate, Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN...\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_batch at 0x7f7a96488b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function parse_batch at 0x7f7a96488b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function parse_batch at 0x7f7a96488b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/150\n",
      "15/15 [==============================] - 3s 89ms/step - loss: 119359.8516 - val_loss: 12192.1670\n",
      "Epoch 2/150\n",
      "15/15 [==============================] - 2s 129ms/step - loss: 217484.5059 - val_loss: 67914.5078\n",
      "Epoch 3/150\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 267766.8672 - val_loss: 178422.7031\n",
      "Epoch 4/150\n",
      "15/15 [==============================] - 3s 175ms/step - loss: 276523.2480 - val_loss: 233676.2500\n",
      "Epoch 5/150\n",
      "15/15 [==============================] - 2s 123ms/step - loss: 226595.2031 - val_loss: 238311.1250\n",
      "Epoch 6/150\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 190567.0781 - val_loss: 241260.7500\n",
      "Epoch 7/150\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 172979.1147 - val_loss: 240183.9219\n",
      "Epoch 8/150\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 143535.2520 - val_loss: 221005.0781\n",
      "Epoch 9/150\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 131618.6848 - val_loss: 233117.1250\n",
      "Epoch 10/150\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 112392.6165 - val_loss: 209661.5000\n",
      "Epoch 11/150\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 91142.3074 - val_loss: 214352.5469\n",
      "Epoch 12/150\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 78727.6313 - val_loss: 229884.2969\n",
      "Epoch 13/150\n",
      "15/15 [==============================] - 3s 168ms/step - loss: 80733.0881 - val_loss: 219752.8594\n",
      "Epoch 14/150\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 91737.9099 - val_loss: 198660.2500\n",
      "Epoch 15/150\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 91889.5186 - val_loss: 201141.2344\n",
      "Epoch 16/150\n",
      "15/15 [==============================] - 2s 137ms/step - loss: 110441.7246 - val_loss: 196641.5156\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_10_layer_call_fn, leaky_re_lu_10_layer_call_and_return_conditional_losses, leaky_re_lu_10_layer_call_fn, leaky_re_lu_10_layer_call_and_return_conditional_losses, leaky_re_lu_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as leaky_re_lu_10_layer_call_fn, leaky_re_lu_10_layer_call_and_return_conditional_losses, leaky_re_lu_10_layer_call_fn, leaky_re_lu_10_layer_call_and_return_conditional_losses, leaky_re_lu_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "# wide and deep model\n",
    "\n",
    "def get_record_defaults(n_features):\n",
    "    zf = tf.zeros(shape=(1,), dtype=tf.float32)\n",
    "    zi = tf.zeros(shape=(1,), dtype=tf.float32)\n",
    "    return [zf]*n_features + [zi]\n",
    "\n",
    "def parse_batch(tf_string):\n",
    "\n",
    "    # first column = y\n",
    "    # columns from 1 to 50 = x1\n",
    "    # columns from 51 to 100 = x2\n",
    "\n",
    "    n_features = 100 + 1\n",
    "\n",
    "    data = tf.io.decode_csv(\n",
    "        tf_string, get_record_defaults(n_features), field_delim='\\t')\n",
    "\n",
    "    labels = data[0]\n",
    "\n",
    "    x1 = tf.stack(data[1:51], axis=-1) / 255.\n",
    "    x2 = tf.stack(data[51:101], axis=-1) / 255.\n",
    "\n",
    "    return (x1, x2), labels\n",
    "\n",
    "\n",
    "def get_batched_dataset(file_name, batch_size):\n",
    "    dataset = tf.data.TextLineDataset([file_name])\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "    dataset = dataset.map(parse_batch)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model():\n",
    "\n",
    "    # first model for x1\n",
    "    first_input = Input((50, ))\n",
    "    first_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                        activation=activation)(first_input)\n",
    "    first_dense = Dropout(0.5)(first_dense)\n",
    "    first_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                        activation=activation)(first_dense)\n",
    "\n",
    "    # second model for x2\n",
    "    second_input = Input((50, ))\n",
    "    second_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                         activation=activation)(second_input)\n",
    "    second_dense = Dropout(0.5)(second_dense)\n",
    "    second_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                         activation=activation)(second_dense)\n",
    "\n",
    "    # wide model x1 and x2\n",
    "    wide_input = Concatenate()(\n",
    "        [first_input, second_input])\n",
    "    wide_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                       activation=activation)(wide_input)\n",
    "    wide_dense = Dropout(0.5)(wide_dense)\n",
    "    wide_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                       activation=activation)(wide_dense)\n",
    "\n",
    "    # concatenate models\n",
    "    merged = Concatenate()(\n",
    "        [first_dense, second_dense, wide_dense])\n",
    "\n",
    "    output_layer = Dense(1, kernel_initializer='he_normal',\n",
    "                         activation='relu')(merged)\n",
    "\n",
    "    model = Model(inputs=[first_input, second_input], outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('Training ANN...')\n",
    "\n",
    "\n",
    "def n_rows(file_name):\n",
    "    counter = 0\n",
    "    with open(file_name, buffering=2**14) as file:\n",
    "        for _ in file:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "file_name_train = 'dummy_train.csv'\n",
    "num_rows_train = n_rows(file_name_train)\n",
    "\n",
    "file_name_test = 'dummy_test.csv'\n",
    "num_rows_test = n_rows(file_name_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "optimizer = Adam(clipnorm=1.0)\n",
    "\n",
    "activation = LeakyReLU(alpha=0.05)\n",
    "\n",
    "estimator = model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               mode='min',\n",
    "                               verbose=1,\n",
    "                               patience=15,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "history = estimator.fit(get_batched_dataset(file_name_train, batch_size).repeat(),\n",
    "                        steps_per_epoch=num_rows_train // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=get_batched_dataset(\n",
    "                            file_name_test, batch_size).repeat(),\n",
    "                        validation_steps=num_rows_test // batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "\n",
    "estimator.save('model', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf8f56cd42c7583847b490f37abd388845e5f0fc5aa40338065d1d06930cc0b6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
