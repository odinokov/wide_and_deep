{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataset(name:str, n:int=10):\n",
    "\n",
    "    x1 = np.random.randint(0,10,[n,3]) / 10\n",
    "    x2 = np.random.randint(0,10,[n,3]) / 10\n",
    "\n",
    "    y = np.sum(x1*x2, axis=1, keepdims=True)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate((y, x1, x2), axis=1))\n",
    "\n",
    "    # first columns is \"y\"\n",
    "    df.rename(columns={0:'y'}, inplace=True)\n",
    "\n",
    "    df.to_csv(name, sep='\\t', header=None, index=None)\n",
    "\n",
    "make_dataset('dummy_train.csv', 10000)\n",
    "make_dataset('dummy_test.csv', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Concatenate, Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN...\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_batch at 0x7fa229a2b550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function parse_batch at 0x7fa229a2b550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function parse_batch at 0x7fa229a2b550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/150\n",
      "156/156 [==============================] - 13s 73ms/step - loss: 0.2348 - val_loss: 0.0390\n",
      "Epoch 2/150\n",
      "156/156 [==============================] - 14s 92ms/step - loss: 0.0710 - val_loss: 0.0351\n",
      "Epoch 3/150\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.0514 - val_loss: 0.0399\n",
      "Epoch 4/150\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 0.0415 - val_loss: 0.0422\n",
      "Epoch 5/150\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0362 - val_loss: 0.0436\n",
      "Epoch 6/150\n",
      "156/156 [==============================] - 7s 44ms/step - loss: 0.0320 - val_loss: 0.0500\n",
      "Epoch 7/150\n",
      "156/156 [==============================] - 11s 71ms/step - loss: 0.0274 - val_loss: 0.0623\n",
      "Epoch 8/150\n",
      "156/156 [==============================] - 12s 78ms/step - loss: 0.0250 - val_loss: 0.0518\n",
      "Epoch 9/150\n",
      "156/156 [==============================] - 11s 72ms/step - loss: 0.0228 - val_loss: 0.0472\n",
      "Epoch 10/150\n",
      "156/156 [==============================] - 15s 96ms/step - loss: 0.0203 - val_loss: 0.0492\n",
      "Epoch 11/150\n",
      "115/156 [=====================>........] - ETA: 4s - loss: 0.0187"
     ]
    }
   ],
   "source": [
    "# wide and deep model\n",
    "\n",
    "def get_record_defaults(n_features):\n",
    "    zf = tf.zeros(shape=(1,), dtype=tf.float32)\n",
    "    zi = tf.zeros(shape=(1,), dtype=tf.float32)\n",
    "    return [zi] + [zf]*n_features\n",
    "\n",
    "def parse_batch(tf_string):\n",
    "\n",
    "    # zero column = y\n",
    "    # columns from 1 to 3 = x1\n",
    "    # columns from 4 to 6 = x2\n",
    "\n",
    "    n_features = 3 + 3 \n",
    "\n",
    "    data = tf.io.decode_csv(\n",
    "        tf_string, get_record_defaults(n_features), field_delim='\\t')\n",
    "\n",
    "    labels = data[0]\n",
    "\n",
    "    x1 = tf.stack(data[1:4], axis=-1)\n",
    "    x2 = tf.stack(data[4:7], axis=-1)\n",
    "\n",
    "    return (x1, x2), labels\n",
    "\n",
    "\n",
    "def get_batched_dataset(file_name, batch_size):\n",
    "    dataset = tf.data.TextLineDataset([file_name])\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "    dataset = dataset.map(parse_batch)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model():\n",
    "\n",
    "    # first model for x1\n",
    "    first_input = Input((3, ))\n",
    "    first_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                        activation=activation)(first_input)\n",
    "    first_dense = Dropout(0.5)(first_dense)\n",
    "    first_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                        activation=activation)(first_dense)\n",
    "\n",
    "    # second model for x2\n",
    "    second_input = Input((3, ))\n",
    "    second_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                         activation=activation)(second_input)\n",
    "    second_dense = Dropout(0.5)(second_dense)\n",
    "    second_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                         activation=activation)(second_dense)\n",
    "\n",
    "    # wide model x1 and x2\n",
    "    wide_input = Concatenate()(\n",
    "        [first_input, second_input])\n",
    "    wide_dense = Dense(32, kernel_initializer='he_normal',\n",
    "                       activation=activation)(wide_input)\n",
    "    wide_dense = Dropout(0.5)(wide_dense)\n",
    "    wide_dense = Dense(16, kernel_initializer='he_normal',\n",
    "                       activation=activation)(wide_dense)\n",
    "\n",
    "    # concatenate models\n",
    "    merged = Concatenate()(\n",
    "        [first_dense, second_dense, wide_dense])\n",
    "    output_layer = Dense(32, kernel_initializer='he_normal',\n",
    "                         activation=activation)(merged)\n",
    "    output_layer = Dense(1, kernel_initializer='he_normal',\n",
    "                       activation='relu')(output_layer)\n",
    "\n",
    "    model = Model(inputs=[first_input, second_input], outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('Training ANN...')\n",
    "\n",
    "\n",
    "def n_rows(file_name):\n",
    "    counter = 0\n",
    "    with open(file_name, buffering=2**14) as file:\n",
    "        for _ in file:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "file_name_train = 'dummy_train.csv'\n",
    "num_rows_train = n_rows(file_name_train)\n",
    "\n",
    "file_name_test = 'dummy_test.csv'\n",
    "num_rows_test = n_rows(file_name_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "optimizer = Adam(clipnorm=1.0)\n",
    "\n",
    "activation = LeakyReLU(alpha=0.05)\n",
    "\n",
    "estimator = model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               mode='min',\n",
    "                               verbose=1,\n",
    "                               patience=15,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "history = estimator.fit(get_batched_dataset(file_name_train, batch_size).repeat(),\n",
    "                        steps_per_epoch=num_rows_train // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=get_batched_dataset(\n",
    "                            file_name_test, batch_size).repeat(),\n",
    "                        validation_steps=num_rows_test // batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "\n",
    "estimator.save('model', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf8f56cd42c7583847b490f37abd388845e5f0fc5aa40338065d1d06930cc0b6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
